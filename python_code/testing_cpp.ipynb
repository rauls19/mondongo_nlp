{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARSE MATRIX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE - 1\n",
    "docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "vocabulary = {}\n",
    "for d in docs:\n",
    "    for term in d:\n",
    "        index = vocabulary.setdefault(term, len(vocabulary))\n",
    "        indices.append(index)\n",
    "        data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "csr_mat = scipy.sparse.csr_matrix((data, indices, indptr), dtype=int)\n",
    "print(\"indptr \", csr_mat.indptr)\n",
    "print(\"column \", csr_mat.indices)\n",
    "print(\"shape \", csr_mat.shape)\n",
    "print(\"data \", csr_mat.data)\n",
    "print(\"csr\")\n",
    "print(csr_mat)\n",
    "print(\"array\")\n",
    "print(csr_mat.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CASE - 2\n",
    "csr_mat = scipy.sparse.csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
    "print(\"indptr \", csr_mat.indptr)\n",
    "print(\"column \", csr_mat.indices)\n",
    "print(\"shape \", csr_mat.shape)\n",
    "print(\"data \", csr_mat.data)\n",
    "print(\"csr\")\n",
    "print(csr_mat)\n",
    "print(\"array\")\n",
    "print(csr_mat.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "corpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.75)\n",
    "X = vectorizer.fit(corpus).transform(corpus)\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.indptr)\n",
    "print(X.data)\n",
    "print(X.indices)\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('description_b.csv', sep=';')\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIXED TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "tf = 1 / 1\n",
    "n = 4 # number of documents\n",
    "df_t = 3\n",
    "idf = math.log((1 + n) / (1 + df_t)) + 1\n",
    "print(\"tf \", tf)\n",
    "print(\"idf \",idf)\n",
    "tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/1.22314 * 0.815429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('books_data.csv', sep=',')\n",
    "df2 = pd.read_csv('bundles_summary.csv', sep=',')\n",
    "df3 = pd.read_csv('wiki_movie_plots_deduped.csv', sep=',')\n",
    "df1 = df1[['description']].copy()\n",
    "df1.dropna(inplace=True)\n",
    "df1.drop_duplicates(inplace=True)\n",
    "df2 = df2[[\"summary\"]].copy()\n",
    "df2.dropna(inplace=True)\n",
    "df2.drop_duplicates(inplace=True)\n",
    "df3 = df3[[\"Plot\"]].copy()\n",
    "df3.dropna(inplace=True)\n",
    "df3.drop_duplicates(inplace=True)\n",
    "df1.columns = [\"text\"]\n",
    "df2.columns = [\"text\"]\n",
    "df3.columns = [\"text\"]\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df.to_csv(\"description_b.csv\", sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7315e4bc1611c656b2d62fa3cdf5af709677cadf7799c202d89a7466c101a0d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
